{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--mode MODE] [--env ENV]\n",
      "                             [--start_steps START_STEPS] [--actor_lr ACTOR_LR]\n",
      "                             [--critic_lr CRITIC_LR] [--batch_size BATCH_SIZE]\n",
      "                             [--discount DISCOUNT]\n",
      "                             [--reward_scale REWARD_SCALE] [--tau TAU]\n",
      "                             [--layer_norm] [--use_td3]\n",
      "                             [--policy_noise POLICY_NOISE]\n",
      "                             [--noise_clip NOISE_CLIP]\n",
      "                             [--policy_freq POLICY_FREQ]\n",
      "                             [--gauss_sigma GAUSS_SIGMA] [--ou_noise]\n",
      "                             [--ou_theta OU_THETA] [--ou_sigma OU_SIGMA]\n",
      "                             [--ou_mu OU_MU] [--pop_size POP_SIZE] [--elitism]\n",
      "                             [--n_grad N_GRAD] [--sigma_init SIGMA_INIT]\n",
      "                             [--damp DAMP] [--damp_limit DAMP_LIMIT]\n",
      "                             [--mult_noise] [--n_episodes N_EPISODES]\n",
      "                             [--max_steps MAX_STEPS] [--mem_size MEM_SIZE]\n",
      "                             [--n_noisy N_NOISY] [--filename FILENAME]\n",
      "                             [--n_test N_TEST] [--output OUTPUT]\n",
      "                             [--period PERIOD] [--n_eval N_EVAL]\n",
      "                             [--save_all_models] [--debug] [--seed SEED]\n",
      "                             [--render]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/colinelacoux/Library/Jupyter/runtime/kernel-3e50d937-a11c-4edc-a405-bed59c11a24a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colinelacoux/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ES import sepCEM, Control\n",
    "from models import RLNN\n",
    "from random_process import GaussianNoise, OrnsteinUhlenbeckProcess\n",
    "from memory import Memory\n",
    "from util import *\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "\n",
    "save_average_mu_in_csv = True #enregistrer les différents mu average de l'actor dans un csv pendant l'apprentissage\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "else:\n",
    "    FloatTensor = torch.FloatTensor #?\n",
    "\n",
    "\n",
    "def evaluate(actor, env, memory=None, n_episodes=1, random=False, noise=None, render=False):\n",
    "    \"\"\"\n",
    "    Computes the score of an actor on a given number of runs,\n",
    "    fills the memory if needed\n",
    "    \"\"\"\n",
    "\n",
    "    if not random:\n",
    "        def policy(state): # Prise de décision de l'action à effectuer => ça nous intéresse\n",
    "            state = FloatTensor(state.reshape(-1))\n",
    "            action = actor(state).cpu().data.numpy().flatten() # ???\n",
    "\n",
    "            if noise is not None:\n",
    "                action += noise.sample()\n",
    "\n",
    "            return np.clip(action, -max_action, max_action)\n",
    "\n",
    "    else:\n",
    "        def policy(state):\n",
    "            return env.action_space.sample()\n",
    "\n",
    "    scores = []\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "\n",
    "        score = 0\n",
    "        obs = deepcopy(env.reset())\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # get next action and act\n",
    "            action = policy(obs)\n",
    "            n_obs, reward, done, _ = env.step(action)\n",
    "            done_bool = 0 if steps + \\\n",
    "                1 == env._max_episode_steps else float(done)\n",
    "            score += reward\n",
    "            steps += 1\n",
    "\n",
    "            # adding in memory\n",
    "            if memory is not None:\n",
    "                memory.add((obs, n_obs, action, reward, done_bool))\n",
    "            obs = n_obs\n",
    "\n",
    "            # render if needed\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # reset when done\n",
    "            if done:\n",
    "                env.reset()\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores), steps\n",
    "\n",
    "\n",
    "class Actor(RLNN):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action, args):\n",
    "        super(Actor, self).__init__(state_dim, action_dim, max_action)\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        if args.layer_norm:\n",
    "            self.n1 = nn.LayerNorm(400)\n",
    "            self.n2 = nn.LayerNorm(300)\n",
    "        self.layer_norm = args.layer_norm\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=args.actor_lr)\n",
    "        self.tau = args.tau\n",
    "        self.discount = args.discount\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if not self.layer_norm:\n",
    "            x = torch.tanh(self.l1(x))\n",
    "            x = torch.tanh(self.l2(x))\n",
    "            x = self.max_action * torch.tanh(self.l3(x))\n",
    "\n",
    "        else:\n",
    "            x = torch.tanh(self.n1(self.l1(x)))\n",
    "            x = torch.tanh(self.n2(self.l2(x)))\n",
    "            x = self.max_action * torch.tanh(self.l3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def update(self, memory, batch_size, critic, actor_t):\n",
    "\n",
    "        # Sample replay buffer\n",
    "        states, _, _, _, _ = memory.sample(batch_size)\n",
    "\n",
    "        # Compute actor loss\n",
    "        if args.use_td3:\n",
    "            actor_loss = -critic(states, self(states))[0].mean()\n",
    "        else:\n",
    "            actor_loss = -critic(states, self(states)).mean()\n",
    "\n",
    "        # Optimize the actor\n",
    "        self.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.parameters(), actor_t.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "class Critic(RLNN):\n",
    "    def __init__(self, state_dim, action_dim, max_action, args):\n",
    "        super(Critic, self).__init__(state_dim, action_dim, 1)\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 40)\n",
    "        self.l2 = nn.Linear(40, 30)\n",
    "        self.l3 = nn.Linear(30, 1)\n",
    "\n",
    "        if args.layer_norm:\n",
    "            self.n1 = nn.LayerNorm(40)\n",
    "            self.n2 = nn.LayerNorm(30)\n",
    "\n",
    "        self.layer_norm = args.layer_norm\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=args.critic_lr)\n",
    "        self.tau = args.tau\n",
    "        self.discount = args.discount\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x, u):\n",
    "\n",
    "        if not self.layer_norm:\n",
    "            x = F.leaky_relu(self.l1(torch.cat([x, u], 1)))\n",
    "            x = F.leaky_relu(self.l2(x))\n",
    "            x = self.l3(x)\n",
    "\n",
    "        else:\n",
    "            x = F.leaky_relu(self.n1(self.l1(torch.cat([x, u], 1))))\n",
    "            x = F.leaky_relu(self.n2(self.l2(x)))\n",
    "            x = self.l3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def update(self, memory, batch_size, actor_t, critic_t):\n",
    "\n",
    "        # Sample replay buffer\n",
    "        states, n_states, actions, rewards, dones = memory.sample(batch_size)\n",
    "\n",
    "        # Q target = reward + discount * Q(next_state, pi(next_state))\n",
    "        with torch.no_grad():\n",
    "            target_Q = critic_t(n_states, actor_t(n_states))\n",
    "            target_Q = rewards + (1 - dones) * self.discount * target_Q\n",
    "\n",
    "        # Get current Q estimate\n",
    "        current_Q = self(states, actions)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.parameters(), critic_t.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "class CriticTD3(RLNN):\n",
    "    def __init__(self, state_dim, action_dim, max_action, args):\n",
    "        super(CriticTD3, self).__init__(state_dim, action_dim, 1)\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 40)\n",
    "        self.l2 = nn.Linear(40, 30)\n",
    "        self.l3 = nn.Linear(30, 1)\n",
    "\n",
    "        if args.layer_norm:\n",
    "            self.n1 = nn.LayerNorm(40)\n",
    "            self.n2 = nn.LayerNorm(30)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 40)\n",
    "        self.l5 = nn.Linear(40, 30)\n",
    "        self.l6 = nn.Linear(30, 1)\n",
    "\n",
    "        if args.layer_norm:\n",
    "            self.n4 = nn.LayerNorm(40)\n",
    "            self.n5 = nn.LayerNorm(30)\n",
    "\n",
    "        self.layer_norm = args.layer_norm\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=args.critic_lr)\n",
    "        self.tau = args.tau\n",
    "        self.discount = args.discount\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "        self.policy_noise = args.policy_noise\n",
    "        self.noise_clip = args.noise_clip\n",
    "\n",
    "    def forward(self, x, u):\n",
    "\n",
    "        if not self.layer_norm:\n",
    "            x1 = F.leaky_relu(self.l1(torch.cat([x, u], 1)))\n",
    "            x1 = F.leaky_relu(self.l2(x1))\n",
    "            x1 = self.l3(x1)\n",
    "\n",
    "        else:\n",
    "            x1 = F.leaky_relu(self.n1(self.l1(torch.cat([x, u], 1))))\n",
    "            x1 = F.leaky_relu(self.n2(self.l2(x1)))\n",
    "            x1 = self.l3(x1)\n",
    "\n",
    "        if not self.layer_norm:\n",
    "            x2 = F.leaky_relu(self.l4(torch.cat([x, u], 1)))\n",
    "            x2 = F.leaky_relu(self.l5(x2))\n",
    "            x2 = self.l6(x2)\n",
    "\n",
    "        else:\n",
    "            x2 = F.leaky_relu(self.n4(self.l4(torch.cat([x, u], 1))))\n",
    "            x2 = F.leaky_relu(self.n5(self.l5(x2)))\n",
    "            x2 = self.l6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def update(self, memory, batch_size, actor_t, critic_t):\n",
    "\n",
    "        # Sample replay buffer\n",
    "        states, n_states, actions, rewards, dones = memory.sample(batch_size)\n",
    "\n",
    "        # Select action according to policy and add clipped noise\n",
    "        noise = np.clip(np.random.normal(0, self.policy_noise, size=(\n",
    "            batch_size, action_dim)), -self.noise_clip, self.noise_clip)\n",
    "        n_actions = actor_t(n_states) + FloatTensor(noise)\n",
    "        n_actions = n_actions.clamp(-max_action, max_action)\n",
    "\n",
    "        # Q target = reward + discount * min_i(Qi(next_state, pi(next_state)))\n",
    "        with torch.no_grad():\n",
    "            target_Q1, target_Q2 = critic_t(n_states, n_actions)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = rewards + (1 - dones) * self.discount * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self(states, actions)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = nn.MSELoss()(current_Q1, target_Q) + \\\n",
    "            nn.MSELoss()(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.parameters(), critic_t.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--mode', default='train', type=str,)\n",
    "    parser.add_argument('--env', default='Swimmer-v2', type=str)\n",
    "    parser.add_argument('--start_steps', default=10000, type=int)\n",
    "\n",
    "    # DDPG parameters\n",
    "    parser.add_argument('--actor_lr', default=0.001, type=float)\n",
    "    parser.add_argument('--critic_lr', default=0.001, type=float)\n",
    "    parser.add_argument('--batch_size', default=100, type=int)\n",
    "    parser.add_argument('--discount', default=0.99, type=float)\n",
    "    parser.add_argument('--reward_scale', default=1., type=float)\n",
    "    parser.add_argument('--tau', default=0.005, type=float)\n",
    "    parser.add_argument('--layer_norm', dest='layer_norm', action='store_true')\n",
    "\n",
    "    # TD3 parameters\n",
    "    parser.add_argument('--use_td3', dest='use_td3', action='store_false')\n",
    "    parser.add_argument('--policy_noise', default=0.2, type=float)\n",
    "    parser.add_argument('--noise_clip', default=0.5, type=float)\n",
    "    parser.add_argument('--policy_freq', default=2, type=int)\n",
    "\n",
    "    # Gaussian noise parameters\n",
    "    parser.add_argument('--gauss_sigma', default=0.1, type=float)\n",
    "\n",
    "    # OU process parameters\n",
    "    parser.add_argument('--ou_noise', dest='ou_noise', action='store_true')\n",
    "    parser.add_argument('--ou_theta', default=0.15, type=float)\n",
    "    parser.add_argument('--ou_sigma', default=0.2, type=float)\n",
    "    parser.add_argument('--ou_mu', default=0.0, type=float)\n",
    "\n",
    "    # ES parameters\n",
    "    parser.add_argument('--pop_size', default=10, type=int)\n",
    "    parser.add_argument('--elitism', dest=\"elitism\",  action='store_true')\n",
    "    parser.add_argument('--n_grad', default=5, type=int)\n",
    "    parser.add_argument('--sigma_init', default=1e-3, type=float)\n",
    "    parser.add_argument('--damp', default=1e-3, type=float)\n",
    "    parser.add_argument('--damp_limit', default=1e-5, type=float)\n",
    "    parser.add_argument('--mult_noise', dest='mult_noise', action='store_true')\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument('--n_episodes', default=1, type=int)\n",
    "    parser.add_argument('--max_steps', default=500000, type=int)\n",
    "    parser.add_argument('--mem_size', default=1000000, type=int)\n",
    "    parser.add_argument('--n_noisy', default=0, type=int)\n",
    "\n",
    "    # Testing parameters\n",
    "    parser.add_argument('--filename', default=\"\", type=str)\n",
    "    parser.add_argument('--n_test', default=1, type=int)\n",
    "\n",
    "    # misc\n",
    "    parser.add_argument('--output', default='results/', type=str)\n",
    "    parser.add_argument('--period', default=5000, type=int)\n",
    "    parser.add_argument('--n_eval', default=10, type=int)\n",
    "    parser.add_argument('--save_all_models',\n",
    "                        dest=\"save_all_models\", action=\"store_true\")\n",
    "    parser.add_argument('--debug', dest='debug', action='store_true')\n",
    "    parser.add_argument('--seed', default=-1, type=int)\n",
    "    parser.add_argument('--render', dest='render', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \"\"\"args.output = get_output_folder(args.output, args.env)\n",
    "    with open(args.output + \"/parameters.txt\", 'w') as file:\n",
    "        for key, value in vars(args).items():\n",
    "            file.write(\"{} = {}\\n\".format(key, value))\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"creating environment\")\n",
    "    # environment\n",
    "    env = gym.make(args.env)\n",
    "\n",
    "    print(\"starting...\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = int(env.action_space.high[0])\n",
    "\n",
    "    # memory\n",
    "    memory = Memory(args.mem_size, state_dim, action_dim)\n",
    "\n",
    "    data = []\n",
    "    fitness = []\n",
    "\n",
    "    for i in range(1000, 10000, 1000):\n",
    "        print(i)\n",
    "        model = Actor(state_dim, action_dim, max_action, args)\n",
    "        model.load_model(\"testGeneration\",\"iter_\"+str(i))\n",
    "        params = model.get_params()\n",
    "        fit = evaluate(model, env, memory=memory, n_episodes=1, random=False, noise=None, render=False)\n",
    "        data.append(params)\n",
    "        fitness.append(fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    print(np.array(data).shape)\n",
    "\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(data)\n",
    "    print(data)\n",
    "    print(tsne_results)\n",
    "    x = tsne_results[:,0]\n",
    "    print(\"x : \", x)\n",
    "    y = tsne_results[:,1]\n",
    "    print(\"y : \", y)\n",
    "    plt.scatter(x , y , color = fitness) #, c=fitness, cmap='viridis'\n",
    "    plt.show()\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
